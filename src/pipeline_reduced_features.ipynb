{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b928f0ba-64ec-41a1-b274-121588a4a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import pickle\n",
    "import xgboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from mlxtend.evaluate.time_series import (\n",
    "    GroupTimeSeriesSplit,\n",
    "    plot_splits,\n",
    "    print_cv_info,\n",
    "    print_split_info,\n",
    ")\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899eae9c-b091-4d9f-b528-2a7b6df4fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data_folder = '../data/preprocessed/'\n",
    "\n",
    "with open(os.path.join(data_folder, 'train_val_dataset.pkl'), 'rb') as file:\n",
    "    train_val_dataset = pickle.load(file)\n",
    "    \n",
    "with open(os.path.join(data_folder, 'test_dataset.pkl'), 'rb') as file:\n",
    "    test_dataset = pickle.load(file)\n",
    "\n",
    "X_other, y_other, groups_other = train_val_dataset['X_other'], train_val_dataset['y_other'], train_val_dataset['groups_other']\n",
    "X_test, y_test, groups_test, submission_id = test_dataset['X_test'], test_dataset['y_test'], test_dataset['groups_test'], test_dataset['submission_id']\n",
    "\n",
    "# collect which encoder to use on each feature\n",
    "onehot_ftrs = ['imbalance_buy_sell_flag', 'stock_id']\n",
    "std_ftrs = ['seconds_in_bucket', 'imbalance_size', 'reference_price', 'matched_size', \n",
    "            'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', \n",
    "            'wap', 'lagged_target_1d_0', 'lagged_target_1d_10', 'lagged_target_1d_20', \n",
    "            'lagged_target_1d_30', 'lagged_target_1d_40', 'lagged_target_1d_50', \n",
    "            'lagged_target_1d_60', 'lagged_target_1d_70', 'lagged_target_1d_80', \n",
    "            'lagged_target_1d_90', 'lagged_target_1d_100', 'lagged_target_1d_110', \n",
    "            'lagged_target_1d_120', 'lagged_target_1d_130', 'lagged_target_1d_140', \n",
    "            'lagged_target_1d_150', 'lagged_target_1d_160', 'lagged_target_1d_170', \n",
    "            'lagged_target_1d_180', 'lagged_target_1d_190', 'lagged_target_1d_200', \n",
    "            'lagged_target_1d_210', 'lagged_target_1d_220', 'lagged_target_1d_230', \n",
    "            'lagged_target_1d_240', 'lagged_target_1d_250', 'lagged_target_1d_260', \n",
    "            'lagged_target_1d_270', 'lagged_target_1d_280', 'lagged_target_1d_290', \n",
    "            'lagged_target_1d_300', 'lagged_target_1d_310', 'lagged_target_1d_320', \n",
    "            'lagged_target_1d_330', 'lagged_target_1d_340', 'lagged_target_1d_350', \n",
    "            'lagged_target_1d_360', 'lagged_target_1d_370', 'lagged_target_1d_380', \n",
    "            'lagged_target_1d_390', 'lagged_target_1d_400', 'lagged_target_1d_410', \n",
    "            'lagged_target_1d_420', 'lagged_target_1d_430', 'lagged_target_1d_440', \n",
    "            'lagged_target_1d_450', 'lagged_target_1d_460', 'lagged_target_1d_470', \n",
    "            'lagged_target_1d_480', 'lagged_target_1d_490', 'lagged_target_1d_500', \n",
    "            'lagged_target_1d_510', 'lagged_target_1d_520', 'lagged_target_1d_530', \n",
    "            'lagged_target_1d_540', 'volume', 'mid_price', 'liquidity_imbalance', \n",
    "            'matched_imbalance', 'size_imbalance', 'reference_price_far_price_imb', \n",
    "            'reference_price_near_price_imb', 'reference_price_ask_price_imb', \n",
    "            'reference_price_bid_price_imb', 'reference_price_wap_imb', 'far_price_near_price_imb', \n",
    "            'far_price_ask_price_imb', 'far_price_bid_price_imb', 'far_price_wap_imb', \n",
    "            'near_price_ask_price_imb', 'near_price_bid_price_imb', 'near_price_wap_imb', \n",
    "            'ask_price_bid_price_imb', 'ask_price_wap_imb', 'bid_price_wap_imb', 'price_spread', \n",
    "            'price_pressure', 'market_urgency', 'depth_pressure', 'all_prices_mean', \n",
    "            'all_sizes_mean', 'all_prices_std', 'all_sizes_std', 'all_prices_skew', \n",
    "            'all_sizes_skew', 'all_prices_kurt', 'all_sizes_kurt', 'dow', 'seconds', 'minute']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "result_path = '../result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce43f095-8138-48e6-b218-21aea0fdd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLpipe_reduced_feature(X_other, y_other, groups_other, X_test, y_test, groups_test,\n",
    "                           preprocessor, model, param_grid, path, model_Name):\n",
    "    prep = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "    n_split = 4\n",
    "    count = 0\n",
    "    test_scores = np.zeros(n_split)\n",
    "\n",
    "    # Split train-val data\n",
    "    len_group_other = groups_other.nunique()\n",
    "    gts = GroupTimeSeriesSplit(test_size=int(len_group_other*0.25), n_splits=n_split)\n",
    "\n",
    "    for i_train, i_val in gts.split(X_other, y_other, groups_other):\n",
    "        print(f'\\nFold {count+1} Reduced Features:')\n",
    "        best_models = []\n",
    "        \n",
    "        print(\"\\t Train index:\", i_train, \"Val index:\", i_val)\n",
    "        print(\"\\t Train size:\", len(i_train), \"Val size:\", len(i_val))\n",
    "        X_train, y_train, groups_train = X_other.iloc[i_train], y_other.iloc[i_train], groups_other.iloc[i_train]\n",
    "        X_val, y_val, groups_val = X_other.iloc[i_val], y_other.iloc[i_val], groups_other.iloc[i_val]\n",
    "        \n",
    "        X_train_preprocessed = prep.fit_transform(X_train)\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        \n",
    "        # parameter Searching\n",
    "        pg = ParameterGrid(param_grid)\n",
    "        scores = np.zeros(len(pg))\n",
    "        \n",
    "        print('\\t Preparing datasets...')\n",
    "        df_train = pd.DataFrame(data = X_train_preprocessed, columns = feature_names, index=y_train.index)\n",
    "        del X_train_preprocessed, X_train\n",
    "        gc.collect()\n",
    "        \n",
    "        X_val_preprocessed = prep.transform(X_val)\n",
    "        df_val = pd.DataFrame(data = X_val_preprocessed, columns = feature_names, index=y_val.index)\n",
    "        del X_val_preprocessed, X_val\n",
    "        gc.collect()\n",
    "        \n",
    "        X_test_preprocessed = prep.transform(X_test)\n",
    "        df_test = pd.DataFrame(data = X_test_preprocessed, columns = feature_names, index=y_test.index)\n",
    "\n",
    "        # Free up memory\n",
    "        del X_test_preprocessed\n",
    "        gc.collect()\n",
    "\n",
    "        # reduced feature\n",
    "        # find all unique patterns of missing value in test set\n",
    "        mask = df_test.isnull()\n",
    "        unique_rows = np.array(np.unique(mask, axis=0))\n",
    "        all_y_test_pred = pd.DataFrame()\n",
    "        print('\\t there are', len(unique_rows), 'unique missing value patterns.')\n",
    "\n",
    "        # divide test sets into subgroups according to the unique patterns\n",
    "        for i in range(len(unique_rows)):\n",
    "            print ('\\t working on unique pattern', i)\n",
    "            ## generate X_test subset that matches the unique pattern i: optimized code\n",
    "            index_subset = df_test[mask.eq(unique_rows[i], axis=1).all(axis=1)].index\n",
    "            sub_X_test = df_test.loc[index_subset] \n",
    "            sub_X_test = sub_X_test[df_test.columns[~unique_rows[i]]] # drop nan columns\n",
    "            sub_y_test = y_test.loc[index_subset]\n",
    "\n",
    "            ## prepare train-val subset\n",
    "            # 1.cut the feature columns that have nans in the according sub_X_test\n",
    "            sub_X_train = df_train[df_train.columns[~unique_rows[i]]].copy()\n",
    "            sub_X_val = df_val[df_val.columns[~unique_rows[i]]].copy()\n",
    "            # 2.cut the rows in the sub_X_train and sub_X_CV that have any nans\n",
    "            sub_X_train = sub_X_train.dropna()\n",
    "            sub_X_val = sub_X_val.dropna()   \n",
    "            # 3.cut the sub_Y_train and sub_y_CV accordingly\n",
    "            sub_y_train = y_train.loc[sub_X_train.index]\n",
    "            sub_y_val = y_val.loc[sub_X_val.index]\n",
    "\n",
    "            # run ML algo\n",
    "            # change to 1-D array\n",
    "            sub_y_train_array = sub_y_train.values.ravel()\n",
    "            sub_y_val_array = sub_y_val.values.ravel()\n",
    "            sub_y_test_array = sub_y_test.values.ravel()\n",
    "\n",
    "            # run model\n",
    "            ML_algo = model\n",
    "            pg = ParameterGrid(param_grid)\n",
    "            val_scores = np.zeros(len(pg))\n",
    "                \n",
    "            for p in range(len(pg)):\n",
    "                params = pg[p]\n",
    "                print('\\t\\t params:', params)\n",
    "                ML_algo.set_params(**params)\n",
    "                ML_algo.fit(sub_X_train, sub_y_train_array)\n",
    "                sub_y_val_pred = ML_algo.predict(sub_X_val)\n",
    "                val_scores[p] = mean_absolute_error(sub_y_val_array, sub_y_val_pred)\n",
    "                print('\\t\\t val_score:', val_scores[p])\n",
    "            \n",
    "            best_params = np.array(pg)[val_scores == np.min(val_scores)]\n",
    "            print(f'\\t best model parameters for pattern {i}:\\n', best_params)\n",
    "            print('\\t corresponding validation score:', np.min(val_scores))\n",
    "                \n",
    "            ML_algo.set_params(**best_params[0])\n",
    "            ML_algo.fit(sub_X_train, sub_y_train_array)\n",
    "            best_models.append(ML_algo)\n",
    "            sub_y_test_pred = pd.DataFrame(ML_algo.predict(sub_X_test), index = sub_y_test.index,\n",
    "                                           columns = ['sub_y_test_pred']) # convert in to data frame\n",
    "            all_y_test_pred = pd.concat([all_y_test_pred, sub_y_test_pred])\n",
    "                    \n",
    "        all_y_test_pred = all_y_test_pred.sort_index()\n",
    "        y_test = y_test.sort_index()\n",
    "\n",
    "        # test mae in one-fold\n",
    "        test_mae = mean_absolute_error(all_y_test_pred, y_test)\n",
    "        test_scores[count] = test_mae\n",
    "        count = count + 1\n",
    "        print('\\t test MAE:', test_mae)\n",
    "        \n",
    "    # mean accuracy for all random states\n",
    "    print(f'overall test mean: {np.mean(test_scores)}')\n",
    "    print(f'overall test std: {np.std(test_scores)}\\n')\n",
    "\n",
    "    # save best models for the last fold\n",
    "    with open(os.path.join(path, f'{model_name}_reduced_feature_test_scores.pkl'), 'wb') as file:\n",
    "        pickle.dump(test_scores, file)\n",
    "\n",
    "    with open(os.path.join(path, f'{model_name}_reduced_feature_best_models.pkl'), 'wb') as file:\n",
    "        pickle.dump(best_models, file)\n",
    "    \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e09532-6027-498f-a62b-e515deb7a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08597f40-7740-4535-bec2-668a0a3d0dc5",
   "metadata": {},
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c8e8e-38c2-4358-a1f4-83270eff9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lasso'\n",
    "model = Lasso(max_iter=100000000)\n",
    "param_grid = {'alpha': np.logspace(-2, 1, 10),\n",
    "              'random_state': [42]}\n",
    "\n",
    "start_time = time.time()\n",
    "best_models = MLpipe_reduced_feature(X_other, y_other, groups_other, X_test, y_test, groups_test,\n",
    "                                     preprocessor, model, param_grid, result_path, model_name)\n",
    "print('LASSO reduced features model running time:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1d2b4-3c67-48cd-bd26-44be44840657",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0b75b-d7ab-4dbd-a801-ddade55b348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ridge'\n",
    "model = Ridge()\n",
    "param_grid = {'alpha': np.logspace(-2, 1, 10),\n",
    "              'random_state': [42]}\n",
    "\n",
    "start_time = time.time()\n",
    "best_models = MLpipe_reduced_feature(X_other, y_other, groups_other, X_test, y_test, groups_test,\n",
    "                                     preprocessor, model, param_grid, result_path, model_name)\n",
    "print('Ridge reduced features model running time:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a9720-36ac-4e78-904b-b49fd9873afc",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69b962-b5bb-4a4f-a132-e0f035053cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'randomforest'\n",
    "model = RandomForestRegressor()\n",
    "param_grid = {'max_features': [0.5, 0.75, 1.0, None],\n",
    "              'max_depth': [1, 5, 7, 11, 13, None],\n",
    "              'n_estimators': [100],\n",
    "              'random_state': [42]}\n",
    "\n",
    "start_time = time.time()\n",
    "best_models = MLpipe_reduced_feature(X_other, y_other, groups_other, X_test, y_test, groups_test,\n",
    "                                     preprocessor, model, param_grid, result_path, model_name)\n",
    "print('RF reduced features model running time:', time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
