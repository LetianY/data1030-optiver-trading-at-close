{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-04T06:48:04.937143Z",
     "iopub.status.busy": "2023-12-04T06:48:04.936581Z",
     "iopub.status.idle": "2023-12-04T06:48:06.147543Z",
     "shell.execute_reply": "2023-12-04T06:48:06.146131Z",
     "shell.execute_reply.started": "2023-12-04T06:48:04.937092Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import gc, time, warnings, joblib, pickle\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T06:48:06.151265Z",
     "iopub.status.busy": "2023-12-04T06:48:06.150169Z",
     "iopub.status.idle": "2023-12-04T06:48:07.502698Z",
     "shell.execute_reply": "2023-12-04T06:48:07.501163Z",
     "shell.execute_reply.started": "2023-12-04T06:48:06.151212Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from mlxtend.evaluate.time_series import (\n",
    "    GroupTimeSeriesSplit,\n",
    "    plot_splits,\n",
    "    print_cv_info,\n",
    "    print_split_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T06:48:07.504876Z",
     "iopub.status.busy": "2023-12-04T06:48:07.504427Z",
     "iopub.status.idle": "2023-12-04T06:48:26.166364Z",
     "shell.execute_reply": "2023-12-04T06:48:26.165338Z",
     "shell.execute_reply.started": "2023-12-04T06:48:07.504836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data_folder = '/kaggle/input/data1030-optiver-trading-at-close/'\n",
    "# data_folder = '../data/preprocessed/'\n",
    "\n",
    "with open(os.path.join(data_folder, 'train_val_dataset.pkl'), 'rb') as file:\n",
    "    train_val_dataset = pickle.load(file)\n",
    "    \n",
    "with open(os.path.join(data_folder, 'test_dataset.pkl'), 'rb') as file:\n",
    "    test_dataset = pickle.load(file)\n",
    "\n",
    "X_other, y_other, groups_other = train_val_dataset['X_other'], train_val_dataset['y_other'], train_val_dataset['groups_other']\n",
    "X_test, y_test, groups_test, submission_id = test_dataset['X_test'], test_dataset['y_test'], test_dataset['groups_test'], test_dataset['submission_id']\n",
    "\n",
    "# collect which encoder to use on each feature\n",
    "onehot_ftrs = ['imbalance_buy_sell_flag', 'stock_id']\n",
    "std_ftrs = ['seconds_in_bucket', 'imbalance_size', 'reference_price', 'matched_size', \n",
    "            'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', \n",
    "            'wap', 'lagged_target_1d_0', 'lagged_target_1d_10', 'lagged_target_1d_20', \n",
    "            'lagged_target_1d_30', 'lagged_target_1d_40', 'lagged_target_1d_50', \n",
    "            'lagged_target_1d_60', 'lagged_target_1d_70', 'lagged_target_1d_80', \n",
    "            'lagged_target_1d_90', 'lagged_target_1d_100', 'lagged_target_1d_110', \n",
    "            'lagged_target_1d_120', 'lagged_target_1d_130', 'lagged_target_1d_140', \n",
    "            'lagged_target_1d_150', 'lagged_target_1d_160', 'lagged_target_1d_170', \n",
    "            'lagged_target_1d_180', 'lagged_target_1d_190', 'lagged_target_1d_200', \n",
    "            'lagged_target_1d_210', 'lagged_target_1d_220', 'lagged_target_1d_230', \n",
    "            'lagged_target_1d_240', 'lagged_target_1d_250', 'lagged_target_1d_260', \n",
    "            'lagged_target_1d_270', 'lagged_target_1d_280', 'lagged_target_1d_290', \n",
    "            'lagged_target_1d_300', 'lagged_target_1d_310', 'lagged_target_1d_320', \n",
    "            'lagged_target_1d_330', 'lagged_target_1d_340', 'lagged_target_1d_350', \n",
    "            'lagged_target_1d_360', 'lagged_target_1d_370', 'lagged_target_1d_380', \n",
    "            'lagged_target_1d_390', 'lagged_target_1d_400', 'lagged_target_1d_410', \n",
    "            'lagged_target_1d_420', 'lagged_target_1d_430', 'lagged_target_1d_440', \n",
    "            'lagged_target_1d_450', 'lagged_target_1d_460', 'lagged_target_1d_470', \n",
    "            'lagged_target_1d_480', 'lagged_target_1d_490', 'lagged_target_1d_500', \n",
    "            'lagged_target_1d_510', 'lagged_target_1d_520', 'lagged_target_1d_530', \n",
    "            'lagged_target_1d_540', 'volume', 'mid_price', 'liquidity_imbalance', \n",
    "            'matched_imbalance', 'size_imbalance', 'reference_price_far_price_imb', \n",
    "            'reference_price_near_price_imb', 'reference_price_ask_price_imb', \n",
    "            'reference_price_bid_price_imb', 'reference_price_wap_imb', 'far_price_near_price_imb', \n",
    "            'far_price_ask_price_imb', 'far_price_bid_price_imb', 'far_price_wap_imb', \n",
    "            'near_price_ask_price_imb', 'near_price_bid_price_imb', 'near_price_wap_imb', \n",
    "            'ask_price_bid_price_imb', 'ask_price_wap_imb', 'bid_price_wap_imb', 'price_spread', \n",
    "            'price_pressure', 'market_urgency', 'depth_pressure', 'all_prices_mean', \n",
    "            'all_sizes_mean', 'all_prices_std', 'all_sizes_std', 'all_prices_skew', \n",
    "            'all_sizes_skew', 'all_prices_kurt', 'all_sizes_kurt', 'dow', 'seconds', 'minute']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "model_save_path = 'result'\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "    \n",
    "result_path = '/kaggle/working/result/'\n",
    "# result_path = '../result/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T06:48:26.169619Z",
     "iopub.status.busy": "2023-12-04T06:48:26.168611Z",
     "iopub.status.idle": "2023-12-04T06:48:26.436410Z",
     "shell.execute_reply": "2023-12-04T06:48:26.435119Z",
     "shell.execute_reply.started": "2023-12-04T06:48:26.169582Z"
    }
   },
   "outputs": [],
   "source": [
    "from  sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T06:48:26.438938Z",
     "iopub.status.busy": "2023-12-04T06:48:26.438110Z",
     "iopub.status.idle": "2023-12-04T06:48:26.468867Z",
     "shell.execute_reply": "2023-12-04T06:48:26.467149Z",
     "shell.execute_reply.started": "2023-12-04T06:48:26.438897Z"
    }
   },
   "outputs": [],
   "source": [
    "def MLpipe_reduced_feature(X_other, y_other, groups_other, X_test, y_test, groups_test,\n",
    "                           preprocessor, model, param_grid, path, model_Name):\n",
    "    prep = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "    n_split = 4\n",
    "    count = 0\n",
    "    test_scores = np.zeros(n_split)\n",
    "\n",
    "    # Split train-val data\n",
    "    len_group_other = groups_other.nunique()\n",
    "    gts = GroupTimeSeriesSplit(test_size=int(len_group_other*0.25), n_splits=n_split)\n",
    "\n",
    "    for i_train, i_val in gts.split(X_other, y_other, groups_other):\n",
    "        print(f'\\nFold {count+1} Reduced Features:')\n",
    "        best_models = []\n",
    "        \n",
    "        print(\"\\t Train index:\", i_train, \"Val index:\", i_val)\n",
    "        print(\"\\t Train size:\", len(i_train), \"Val size:\", len(i_val))\n",
    "        X_train, y_train, groups_train = X_other.iloc[i_train], y_other.iloc[i_train], groups_other.iloc[i_train]\n",
    "        X_val, y_val, groups_val = X_other.iloc[i_val], y_other.iloc[i_val], groups_other.iloc[i_val]\n",
    "        \n",
    "        X_train_preprocessed = prep.fit_transform(X_train)\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        \n",
    "        # parameter Searching\n",
    "        pg = ParameterGrid(param_grid)\n",
    "        scores = np.zeros(len(pg))\n",
    "        \n",
    "        print('\\t Preparing datasets...')\n",
    "        df_train = pd.DataFrame(data = X_train_preprocessed, columns = feature_names, index=y_train.index)\n",
    "        del X_train_preprocessed, X_train\n",
    "        gc.collect()\n",
    "        \n",
    "        X_val_preprocessed = prep.transform(X_val)\n",
    "        df_val = pd.DataFrame(data = X_val_preprocessed, columns = feature_names, index=y_val.index)\n",
    "        del X_val_preprocessed, X_val\n",
    "        gc.collect()\n",
    "        \n",
    "        X_test_preprocessed = prep.transform(X_test)\n",
    "        df_test = pd.DataFrame(data = X_test_preprocessed, columns = feature_names, index=y_test.index)\n",
    "\n",
    "        # Free up memory\n",
    "        del X_test_preprocessed\n",
    "        gc.collect()\n",
    "\n",
    "        # reduced feature\n",
    "        # find all unique patterns of missing value in test set\n",
    "        mask = df_test.isnull()\n",
    "        unique_rows = np.array(np.unique(mask, axis=0))\n",
    "        all_y_test_pred = pd.DataFrame()\n",
    "        print('\\t there are', len(unique_rows), 'unique missing value patterns.')\n",
    "\n",
    "        # divide test sets into subgroups according to the unique patterns\n",
    "        for i in range(len(unique_rows)):\n",
    "            print ('\\t working on unique pattern', i)\n",
    "            ## generate X_test subset that matches the unique pattern i: optimized code\n",
    "            index_subset = df_test[mask.eq(unique_rows[i], axis=1).all(axis=1)].index\n",
    "            sub_X_test = df_test.loc[index_subset] \n",
    "            sub_X_test = sub_X_test[df_test.columns[~unique_rows[i]]] # drop nan columns\n",
    "            sub_y_test = y_test.loc[index_subset]\n",
    "\n",
    "            ## prepare train-val subset\n",
    "            # 1.cut the feature columns that have nans in the according sub_X_test\n",
    "            sub_X_train = df_train[df_train.columns[~unique_rows[i]]].copy()\n",
    "            sub_X_val = df_val[df_val.columns[~unique_rows[i]]].copy()\n",
    "            # 2.cut the rows in the sub_X_train and sub_X_CV that have any nans\n",
    "            sub_X_train = sub_X_train.dropna()\n",
    "            sub_X_val = sub_X_val.dropna()   \n",
    "            # 3.cut the sub_Y_train and sub_y_CV accordingly\n",
    "            sub_y_train = y_train.loc[sub_X_train.index]\n",
    "            sub_y_val = y_val.loc[sub_X_val.index]\n",
    "\n",
    "            # run ML algo\n",
    "            # change to 1-D array\n",
    "            sub_y_train_array = sub_y_train.values.ravel()\n",
    "            sub_y_val_array = sub_y_val.values.ravel()\n",
    "            sub_y_test_array = sub_y_test.values.ravel()\n",
    "\n",
    "            # run model\n",
    "            ML_algo = model\n",
    "            pg = ParameterGrid(param_grid)\n",
    "            val_scores = np.zeros(len(pg))\n",
    "                \n",
    "            for p in range(len(pg)):\n",
    "                params = pg[p]\n",
    "                print('\\t\\t params:', params)\n",
    "                ML_algo.set_params(**params)\n",
    "                ML_algo.fit(sub_X_train, sub_y_train_array)\n",
    "                sub_y_val_pred = ML_algo.predict(sub_X_val)\n",
    "                val_scores[p] = mean_absolute_error(sub_y_val_array, sub_y_val_pred)\n",
    "                print('\\t\\t val_score:', val_scores[p])\n",
    "            \n",
    "            best_params = np.array(pg)[val_scores == np.min(val_scores)]\n",
    "            print(f'\\t best model parameters for pattern {i}:\\n', best_params)\n",
    "            print('\\t corresponding validation score:', np.min(val_scores))\n",
    "                \n",
    "            ML_algo.set_params(**best_params[0])\n",
    "            ML_algo.fit(sub_X_train, sub_y_train_array)\n",
    "            best_models.append(ML_algo)\n",
    "            sub_y_test_pred = pd.DataFrame(ML_algo.predict(sub_X_test), index = sub_y_test.index,\n",
    "                                           columns = ['sub_y_test_pred']) # convert in to data frame\n",
    "            all_y_test_pred = pd.concat([all_y_test_pred, sub_y_test_pred])\n",
    "                    \n",
    "        all_y_test_pred = all_y_test_pred.sort_index()\n",
    "        y_test = y_test.sort_index()\n",
    "\n",
    "        # test mae in one-fold\n",
    "        test_mae = mean_absolute_error(all_y_test_pred, y_test)\n",
    "        test_scores[count] = test_mae\n",
    "        count = count + 1\n",
    "        print('\\t test MAE:', test_mae)\n",
    "        \n",
    "    # mean accuracy for all random states\n",
    "    print(f'overall test mean: {np.mean(test_scores)}')\n",
    "    print(f'overall test std: {np.std(test_scores)}\\n')\n",
    "\n",
    "    # save best models for the last fold\n",
    "    with open(os.path.join(path, f'{model_name}_reduced_feature_test_scores.pkl'), 'wb') as file:\n",
    "        pickle.dump(test_scores, file)\n",
    "\n",
    "    with open(os.path.join(path, f'{model_name}_reduced_feature_best_models.pkl'), 'wb') as file:\n",
    "        pickle.dump(best_models, file)\n",
    "    \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T06:48:26.471344Z",
     "iopub.status.busy": "2023-12-04T06:48:26.470795Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'lasso'\n",
    "model = Lasso(max_iter=100000000)\n",
    "param_grid = {'alpha': np.logspace(-2, 1, 10),\n",
    "              'random_state': [42]}\n",
    "\n",
    "start_time = time.time()\n",
    "best_models = MLpipe_reduced_feature(X_other, y_other, groups_other, X_test, y_test, groups_test,\n",
    "                                     preprocessor, model, param_grid, result_path, model_name)\n",
    "print('LASSO reduced features model running time:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ridge'\n",
    "model = Ridge()\n",
    "param_grid = {'alpha': np.logspace(-2, 1, 10),\n",
    "              'random_state': [42]}\n",
    "\n",
    "start_time = time.time()\n",
    "best_models = MLpipe_reduced_feature(X_other, y_other, groups_other, X_test, y_test, groups_test,\n",
    "                                     preprocessor, model, param_grid, result_path, model_name)\n",
    "print('Ridge reduced features model running time:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'randomforest'\n",
    "model = RandomForestRegressor()\n",
    "param_grid = {'max_features': [0.5, 0.75, 1.0, None],\n",
    "              'max_depth': [1, 5, 7, 11, 13, None],\n",
    "              'n_estimators': [100],\n",
    "              'random_state': [42]}\n",
    "\n",
    "start_time = time.time()\n",
    "best_models = MLpipe_reduced_feature(X_other, y_other, groups_other, X_test, y_test, groups_test,\n",
    "                                     preprocessor, model, param_grid, result_path, model_name)\n",
    "print('RF reduced features model running time:', time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4103751,
     "sourceId": 7118588,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
